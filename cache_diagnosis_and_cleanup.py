#!/usr/bin/env python3
"""
Wordæ¨¡æ¿å ä½ç¬¦ç¼“å­˜é—®é¢˜è¯Šæ–­å’Œæ¸…ç†å·¥å…·
ä¸“é—¨è§£å†³æ¨¡æ¿ä¿®æ”¹åç³»ç»Ÿä»æ˜¾ç¤ºæ—§å ä½ç¬¦çš„é—®é¢˜
"""

import os
import zipfile
import xml.etree.ElementTree as ET
import re
import tempfile
import hashlib
import json
from pathlib import Path
from typing import List, Dict, Any
import time

def analyze_word_template_cache_issue():
    """åˆ†æWordæ¨¡æ¿ç¼“å­˜é—®é¢˜"""
    
    template_path = r"E:\trae\0823åˆåŒ3\ä¸Šæ¸¸è½¦æº-å¹¿å·èˆ¶æºï¼ˆé‡‡è´­ï¼‰.docx"
    
    print("ğŸ” Wordæ¨¡æ¿å ä½ç¬¦ç¼“å­˜é—®é¢˜è¯Šæ–­")
    print("=" * 60)
    print(f"ğŸ“„ åˆ†ææ–‡ä»¶: {template_path}")
    
    if not os.path.exists(template_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {template_path}")
        return
    
    # 1. åˆ†ææ–‡ä»¶åŸºæœ¬ä¿¡æ¯
    analyze_file_info(template_path)
    
    # 2. æå–å¹¶åˆ†æå½“å‰å ä½ç¬¦
    current_placeholders = extract_current_placeholders(template_path)
    
    # 3. æ£€æŸ¥å¯èƒ½çš„ç¼“å­˜é—®é¢˜
    check_cache_issues(template_path, current_placeholders)
    
    # 4. æä¾›è§£å†³æ–¹æ¡ˆ
    provide_solutions(template_path, current_placeholders)

def analyze_file_info(template_path: str):
    """åˆ†ææ–‡ä»¶åŸºæœ¬ä¿¡æ¯"""
    
    print(f"\nğŸ“Š æ–‡ä»¶åŸºæœ¬ä¿¡æ¯:")
    
    # æ–‡ä»¶å¤§å°å’Œä¿®æ”¹æ—¶é—´
    stat = os.stat(template_path)
    file_size = stat.st_size
    modified_time = time.ctime(stat.st_mtime)
    
    print(f"  æ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚")
    print(f"  æœ€åä¿®æ”¹: {modified_time}")
    
    # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
    file_hash = calculate_file_hash(template_path)
    print(f"  æ–‡ä»¶å“ˆå¸Œ: {file_hash}")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„docxæ–‡ä»¶
    try:
        with zipfile.ZipFile(template_path, 'r') as zip_ref:
            file_list = zip_ref.namelist()
            has_document_xml = 'word/document.xml' in file_list
            print(f"  æ–‡æ¡£ç»“æ„: {'âœ… æœ‰æ•ˆ' if has_document_xml else 'âŒ æ— æ•ˆ'}")
            print(f"  å†…éƒ¨æ–‡ä»¶æ•°: {len(file_list)}")
    except Exception as e:
        print(f"  æ–‡æ¡£ç»“æ„: âŒ æŸå ({e})")

def calculate_file_hash(file_path: str) -> str:
    """è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œ"""
    hash_md5 = hashlib.md5()
    with open(file_path, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()

def extract_current_placeholders(template_path: str) -> List[str]:
    """æå–å½“å‰æ¨¡æ¿ä¸­çš„å ä½ç¬¦"""
    
    print(f"\nğŸ¯ å½“å‰æ¨¡æ¿å ä½ç¬¦åˆ†æ:")
    
    placeholders = []
    
    try:
        with tempfile.TemporaryDirectory() as temp_dir:
            # è§£å‹docxæ–‡ä»¶
            with zipfile.ZipFile(template_path, 'r') as zip_ref:
                zip_ref.extractall(temp_dir)
            
            # è¯»å–document.xml
            document_xml_path = os.path.join(temp_dir, 'word', 'document.xml')
            with open(document_xml_path, 'r', encoding='utf-8') as f:
                xml_content = f.read()
            
            print(f"  XMLé•¿åº¦: {len(xml_content):,} å­—ç¬¦")
            
            # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
            text_elements = re.findall(r'<w:t[^>]*>([^<]+)</w:t>', xml_content)
            all_text = ' '.join(text_elements)
            
            print(f"  æ–‡æœ¬é•¿åº¦: {len(all_text):,} å­—ç¬¦")
            
            # æŸ¥æ‰¾å„ç§æ ¼å¼çš„å ä½ç¬¦
            placeholder_patterns = [
                (r'\{\{([^}]+)\}\}', 'åŒèŠ±æ‹¬å· {{}}'),
                (r'\{([^{}]+)\}', 'å•èŠ±æ‹¬å· {}'),
                (r'\[([^\]]+)\]', 'æ–¹æ‹¬å· []'),
            ]
            
            all_placeholders = []
            
            for pattern, description in placeholder_patterns:
                matches = re.findall(pattern, all_text)
                if matches:
                    print(f"  {description}: {len(matches)} ä¸ª")
                    for match in matches[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                        print(f"    - {match}")
                        all_placeholders.append(match)
                    if len(matches) > 10:
                        print(f"    ... è¿˜æœ‰ {len(matches) - 10} ä¸ª")
            
            # æ£€æŸ¥åˆ†å‰²å ä½ç¬¦
            fragmented_placeholders = find_fragmented_placeholders(xml_content)
            if fragmented_placeholders:
                print(f"  åˆ†å‰²å ä½ç¬¦: {len(fragmented_placeholders)} ä¸ª")
                for placeholder in fragmented_placeholders[:5]:
                    print(f"    - {placeholder}")
                all_placeholders.extend(fragmented_placeholders)
            
            placeholders = list(set(all_placeholders))
            print(f"\n  ğŸ“Š æ€»è®¡å”¯ä¸€å ä½ç¬¦: {len(placeholders)} ä¸ª")
            
    except Exception as e:
        print(f"  âŒ å ä½ç¬¦æå–å¤±è´¥: {e}")
    
    return placeholders

def find_fragmented_placeholders(xml_content: str) -> List[str]:
    """æŸ¥æ‰¾è¢«åˆ†å‰²çš„å ä½ç¬¦"""
    
    fragmented = []
    
    # æŸ¥æ‰¾å¯èƒ½çš„åˆ†å‰²æ¨¡å¼
    # æ¨¡å¼1: <w:t>{</w:t>...å…¶ä»–å†…å®¹...<w:t>}</w:t>
    pattern1 = r'<w:t[^>]*>\{[^<]*</w:t>.*?<w:t[^>]*>[^}]*\}</w:t>'
    matches1 = re.findall(pattern1, xml_content, re.DOTALL)
    
    for match in matches1:
        # æå–æ–‡æœ¬å†…å®¹
        text_parts = re.findall(r'<w:t[^>]*>([^<]*)</w:t>', match)
        combined_text = ''.join(text_parts)
        
        # æ£€æŸ¥æ˜¯å¦å½¢æˆå®Œæ•´çš„å ä½ç¬¦
        placeholder_match = re.search(r'\{([^{}]+)\}', combined_text)
        if placeholder_match:
            fragmented.append(placeholder_match.group(1))
    
    return list(set(fragmented))

def check_cache_issues(template_path: str, current_placeholders: List[str]):
    """æ£€æŸ¥å¯èƒ½çš„ç¼“å­˜é—®é¢˜"""
    
    print(f"\nğŸ” ç¼“å­˜é—®é¢˜æ£€æŸ¥:")
    
    # 1. æ£€æŸ¥æµè§ˆå™¨å¯èƒ½çš„ç¼“å­˜
    print(f"  ğŸŒ æµè§ˆå™¨ç¼“å­˜æ£€æŸ¥:")
    print(f"    - æ–‡ä»¶åç›¸åŒå¯èƒ½å¯¼è‡´æµè§ˆå™¨ç¼“å­˜")
    print(f"    - å»ºè®®: æ¸…é™¤æµè§ˆå™¨ç¼“å­˜æˆ–é‡å‘½åæ–‡ä»¶")
    
    # 2. æ£€æŸ¥ç³»ç»Ÿä¸´æ—¶æ–‡ä»¶
    temp_dir = tempfile.gettempdir()
    print(f"  ğŸ“ ç³»ç»Ÿä¸´æ—¶æ–‡ä»¶æ£€æŸ¥:")
    print(f"    - ä¸´æ—¶ç›®å½•: {temp_dir}")
    
    # æŸ¥æ‰¾å¯èƒ½çš„ä¸´æ—¶æ–‡ä»¶
    temp_files = []
    try:
        for file in os.listdir(temp_dir):
            if 'docx' in file.lower() or 'ä¸Šæ¸¸è½¦æº' in file:
                temp_files.append(file)
    except:
        pass
    
    if temp_files:
        print(f"    - å‘ç° {len(temp_files)} ä¸ªå¯èƒ½ç›¸å…³çš„ä¸´æ—¶æ–‡ä»¶")
        for temp_file in temp_files[:5]:
            print(f"      â€¢ {temp_file}")
    else:
        print(f"    - æœªå‘ç°ç›¸å…³ä¸´æ—¶æ–‡ä»¶")
    
    # 3. æ£€æŸ¥æ–‡ä»¶é”å®šçŠ¶æ€
    print(f"  ğŸ”’ æ–‡ä»¶çŠ¶æ€æ£€æŸ¥:")
    try:
        # å°è¯•ä»¥å†™æ¨¡å¼æ‰“å¼€æ–‡ä»¶
        with open(template_path, 'r+b') as f:
            print(f"    - æ–‡ä»¶çŠ¶æ€: âœ… å¯è¯»å†™")
    except PermissionError:
        print(f"    - æ–‡ä»¶çŠ¶æ€: âš ï¸ è¢«é”å®šæˆ–åªè¯»")
    except Exception as e:
        print(f"    - æ–‡ä»¶çŠ¶æ€: âŒ å¼‚å¸¸ ({e})")

def provide_solutions(template_path: str, current_placeholders: List[str]):
    """æä¾›è§£å†³æ–¹æ¡ˆ"""
    
    print(f"\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
    
    print(f"  ğŸ”§ ç«‹å³è§£å†³æ–¹æ¡ˆ:")
    print(f"    1. æ¸…é™¤æµè§ˆå™¨ç¼“å­˜:")
    print(f"       - æŒ‰ Ctrl+Shift+Delete æ¸…é™¤æµè§ˆå™¨ç¼“å­˜")
    print(f"       - æˆ–ä½¿ç”¨æ— ç—•æ¨¡å¼é‡æ–°è®¿é—®ç³»ç»Ÿ")
    
    print(f"    2. é‡å‘½åæ–‡ä»¶:")
    new_filename = generate_new_filename(template_path)
    print(f"       - å°†æ–‡ä»¶é‡å‘½åä¸º: {new_filename}")
    print(f"       - ç„¶åé‡æ–°ä¸Šä¼ åˆ°ç³»ç»Ÿ")
    
    print(f"    3. å¼ºåˆ¶åˆ·æ–°:")
    print(f"       - åœ¨ä¸Šä¼ é¡µé¢æŒ‰ Ctrl+F5 å¼ºåˆ¶åˆ·æ–°")
    print(f"       - ç¡®ä¿é¡µé¢å®Œå…¨é‡æ–°åŠ è½½")
    
    print(f"  ğŸ› ï¸  é«˜çº§è§£å†³æ–¹æ¡ˆ:")
    print(f"    4. éªŒè¯æ–‡æ¡£ä¿®æ”¹:")
    print(f"       - é‡æ–°æ‰“å¼€Wordæ–‡æ¡£ç¡®è®¤ä¿®æ”¹å·²ä¿å­˜")
    print(f"       - æ£€æŸ¥å ä½ç¬¦æ ¼å¼æ˜¯å¦æ­£ç¡®")
    
    print(f"    5. åˆ›å»ºæ–°å‰¯æœ¬:")
    backup_path = create_backup_copy(template_path)
    if backup_path:
        print(f"       - å·²åˆ›å»ºå¤‡ä»½å‰¯æœ¬: {backup_path}")
        print(f"       - ä½¿ç”¨å¤‡ä»½å‰¯æœ¬é‡æ–°ä¸Šä¼ ")
    
    print(f"    6. ç³»ç»Ÿè°ƒè¯•:")
    print(f"       - è®¿é—®ç³»ç»Ÿçš„ /debug-generation é¡µé¢")
    print(f"       - ä½¿ç”¨è¯Šæ–­å·¥å…·æ£€æŸ¥å ä½ç¬¦è¯†åˆ«")

def generate_new_filename(original_path: str) -> str:
    """ç”Ÿæˆæ–°çš„æ–‡ä»¶å"""
    
    path_obj = Path(original_path)
    timestamp = int(time.time())
    new_name = f"{path_obj.stem}_ä¿®æ­£ç‰ˆ_{timestamp}{path_obj.suffix}"
    return new_name

def create_backup_copy(original_path: str) -> str:
    """åˆ›å»ºå¤‡ä»½å‰¯æœ¬"""
    
    try:
        path_obj = Path(original_path)
        backup_name = generate_new_filename(original_path)
        backup_path = path_obj.parent / backup_name
        
        # å¤åˆ¶æ–‡ä»¶
        import shutil
        shutil.copy2(original_path, backup_path)
        
        return str(backup_path)
    except Exception as e:
        print(f"    âŒ åˆ›å»ºå¤‡ä»½å¤±è´¥: {e}")
        return ""

def generate_cache_cleanup_script():
    """ç”Ÿæˆç¼“å­˜æ¸…ç†è„šæœ¬"""
    
    print(f"\nğŸ“ ç”Ÿæˆç¼“å­˜æ¸…ç†è„šæœ¬:")
    
    cleanup_script = """
// æµè§ˆå™¨æ§åˆ¶å°ç¼“å­˜æ¸…ç†è„šæœ¬
// åœ¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·çš„æ§åˆ¶å°ä¸­è¿è¡Œ

console.log('ğŸ§¹ å¼€å§‹æ¸…ç†Wordæ¨¡æ¿ç›¸å…³ç¼“å­˜...');

// 1. æ¸…ç†localStorageä¸­çš„æ¨¡æ¿ç¼“å­˜
const templateKeys = Object.keys(localStorage).filter(key => 
    key.includes('template') || key.includes('placeholder') || key.includes('word')
);

templateKeys.forEach(key => {
    localStorage.removeItem(key);
    console.log(`âœ… å·²æ¸…ç†: ${key}`);
});

// 2. æ¸…ç†sessionStorage
const sessionKeys = Object.keys(sessionStorage).filter(key => 
    key.includes('template') || key.includes('placeholder') || key.includes('word')
);

sessionKeys.forEach(key => {
    sessionStorage.removeItem(key);
    console.log(`âœ… å·²æ¸…ç†: ${key}`);
});

// 3. æ¸…ç†å¯èƒ½çš„ç¼“å­˜API
if ('caches' in window) {
    caches.keys().then(cacheNames => {
        cacheNames.forEach(cacheName => {
            if (cacheName.includes('template') || cacheName.includes('word')) {
                caches.delete(cacheName);
                console.log(`âœ… å·²æ¸…ç†ç¼“å­˜: ${cacheName}`);
            }
        });
    });
}

console.log('ğŸ‰ ç¼“å­˜æ¸…ç†å®Œæˆï¼è¯·åˆ·æ–°é¡µé¢å¹¶é‡æ–°ä¸Šä¼ æ¨¡æ¿ã€‚');
"""
    
    script_path = "browser_cache_cleanup.js"
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(cleanup_script)
    
    print(f"  âœ… å·²ç”Ÿæˆ: {script_path}")
    print(f"  ğŸ“‹ ä½¿ç”¨æ–¹æ³•:")
    print(f"    1. åœ¨æµè§ˆå™¨ä¸­æŒ‰ F12 æ‰“å¼€å¼€å‘è€…å·¥å…·")
    print(f"    2. åˆ‡æ¢åˆ° Console æ ‡ç­¾")
    print(f"    3. å¤åˆ¶å¹¶ç²˜è´´è„šæœ¬å†…å®¹")
    print(f"    4. æŒ‰ Enter æ‰§è¡Œ")

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸ”§ Wordæ¨¡æ¿å ä½ç¬¦ç¼“å­˜é—®é¢˜è§£å†³å·¥å…·")
    print("ä¸“é—¨è§£å†³æ¨¡æ¿ä¿®æ”¹åç³»ç»Ÿä»æ˜¾ç¤ºæ—§å ä½ç¬¦çš„é—®é¢˜")
    print("=" * 80)
    
    # æ‰§è¡Œè¯Šæ–­
    analyze_word_template_cache_issue()
    
    # ç”Ÿæˆæ¸…ç†è„šæœ¬
    generate_cache_cleanup_script()
    
    print(f"\nğŸ¯ æ€»ç»“å»ºè®®:")
    print(f"  1. é¦–å…ˆå°è¯•æ¸…é™¤æµè§ˆå™¨ç¼“å­˜å¹¶é‡æ–°ä¸Šä¼ ")
    print(f"  2. å¦‚æœé—®é¢˜æŒç»­ï¼Œé‡å‘½åæ–‡ä»¶åå†ä¸Šä¼ ")
    print(f"  3. ä½¿ç”¨ç”Ÿæˆçš„æ¸…ç†è„šæœ¬æ¸…é™¤æ‰€æœ‰ç›¸å…³ç¼“å­˜")
    print(f"  4. å¦‚æœä»æœ‰é—®é¢˜ï¼Œå¯èƒ½éœ€è¦æ£€æŸ¥Wordæ–‡æ¡£çš„å®é™…å†…å®¹")
    
    print(f"\nâœ… è¯Šæ–­å®Œæˆ")

if __name__ == '__main__':
    main()
