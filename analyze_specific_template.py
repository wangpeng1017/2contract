#!/usr/bin/env python3
"""
åˆ†æå…·ä½“çš„æ¨¡æ¿æ–‡ä»¶ï¼Œæ‰¾å‡ºå ä½ç¬¦è¯†åˆ«å¤±è´¥çš„åŸå› 
ä¸“é—¨åˆ†æ"ä¸Šæ¸¸è½¦æº-å¹¿å·èˆ¶æºï¼ˆé‡‡è´­ï¼‰.docx"
"""

import os
import zipfile
import xml.etree.ElementTree as ET
import re
from pathlib import Path
import tempfile

def analyze_specific_template():
    """åˆ†æå…·ä½“çš„æ¨¡æ¿æ–‡ä»¶"""
    template_path = r"E:\trae\0814åˆåŒ\ä¸Šæ¸¸è½¦æº-å¹¿å·èˆ¶æºï¼ˆé‡‡è´­ï¼‰.docx"
    
    print("ğŸ” åˆ†æå…·ä½“æ¨¡æ¿æ–‡ä»¶")
    print("=" * 60)
    print(f"ğŸ“„ æ¨¡æ¿è·¯å¾„: {template_path}")
    
    if not os.path.exists(template_path):
        print(f"âŒ æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_path}")
        return
    
    with tempfile.TemporaryDirectory() as temp_dir:
        # è§£å‹docxæ–‡ä»¶
        with zipfile.ZipFile(template_path, 'r') as zip_ref:
            zip_ref.extractall(temp_dir)
        
        # è¯»å–document.xml
        document_xml_path = os.path.join(temp_dir, 'word', 'document.xml')
        with open(document_xml_path, 'r', encoding='utf-8') as f:
            xml_content = f.read()
        
        print(f"ğŸ“„ XMLé•¿åº¦: {len(xml_content):,} å­—ç¬¦")
        
        # æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
        text_elements = re.findall(r'<w:t[^>]*>([^<]+)</w:t>', xml_content)
        all_text = ' '.join(text_elements)
        
        print(f"ğŸ“„ æ–‡æœ¬é•¿åº¦: {len(all_text):,} å­—ç¬¦")
        
        # åˆ†æå ä½ç¬¦
        analyze_placeholders(xml_content, all_text)
        
        # åˆ†æXMLç»“æ„é—®é¢˜
        analyze_xml_structure(xml_content)
        
        # æ¨¡æ‹Ÿç³»ç»Ÿçš„è¯†åˆ«è¿‡ç¨‹
        simulate_system_recognition(xml_content)

def analyze_placeholders(xml_content, all_text):
    """åˆ†æå ä½ç¬¦"""
    print(f"\nğŸ¯ å ä½ç¬¦åˆ†æ:")
    
    # 1. åœ¨çº¯æ–‡æœ¬ä¸­æŸ¥æ‰¾å ä½ç¬¦
    print(f"  ğŸ“ çº¯æ–‡æœ¬ä¸­çš„å ä½ç¬¦:")
    
    # å„ç§å¯èƒ½çš„å ä½ç¬¦æ ¼å¼
    patterns = [
        (r'\{\{([^}]+)\}\}', 'åŒèŠ±æ‹¬å· {{}}'),
        (r'\{([^{}]+)\}', 'å•èŠ±æ‹¬å· {}'),
        (r'\[([^\]]+)\]', 'æ–¹æ‹¬å· []'),
        (r'___+', 'ä¸‹åˆ’çº¿ ___'),
        (r'\.{3,}', 'ç‚¹çº¿ ...'),
    ]
    
    total_found = 0
    for pattern, description in patterns:
        matches = re.findall(pattern, all_text)
        if matches:
            print(f"    {description}: {len(matches)} ä¸ª")
            for match in matches[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                if isinstance(match, str):
                    print(f"      - {match}")
                else:
                    print(f"      - {match}")
            total_found += len(matches)
    
    print(f"  ğŸ“Š çº¯æ–‡æœ¬ä¸­æ€»è®¡æ‰¾åˆ°: {total_found} ä¸ªå ä½ç¬¦")
    
    # 2. åœ¨XMLä¸­ç›´æ¥æŸ¥æ‰¾å ä½ç¬¦
    print(f"\n  ğŸ” XMLä¸­çš„å ä½ç¬¦:")
    
    xml_patterns = [
        (r'\{\{([^}]+)\}\}', 'åŒèŠ±æ‹¬å· {{}}'),
        (r'\{([^{}]+)\}', 'å•èŠ±æ‹¬å· {}'),
    ]
    
    xml_total = 0
    for pattern, description in xml_patterns:
        matches = re.findall(pattern, xml_content)
        if matches:
            print(f"    {description}: {len(matches)} ä¸ª")
            for match in matches[:10]:
                print(f"      - {match}")
            xml_total += len(matches)
    
    print(f"  ğŸ“Š XMLä¸­æ€»è®¡æ‰¾åˆ°: {xml_total} ä¸ªå ä½ç¬¦")
    
    # 3. æŸ¥æ‰¾å¯èƒ½è¢«åˆ†å‰²çš„å ä½ç¬¦
    print(f"\n  ğŸ§© åˆ†å‰²å ä½ç¬¦åˆ†æ:")
    analyze_fragmented_placeholders(xml_content)

def analyze_fragmented_placeholders(xml_content):
    """åˆ†æè¢«åˆ†å‰²çš„å ä½ç¬¦"""
    
    # æŸ¥æ‰¾å¯èƒ½çš„åˆ†å‰²æ¨¡å¼
    fragmented_patterns = [
        r'<w:t[^>]*>\{[^<]*</w:t>.*?<w:t[^>]*>[^}]*\}</w:t>',  # {å¼€å§‹...}ç»“æŸ
        r'<w:t[^>]*>\{\{[^<]*</w:t>.*?<w:t[^>]*>[^}]*\}\}</w:t>',  # {{å¼€å§‹...}}ç»“æŸ
    ]
    
    found_fragments = []
    for pattern in fragmented_patterns:
        matches = re.findall(pattern, xml_content, re.DOTALL)
        found_fragments.extend(matches)
    
    if found_fragments:
        print(f"    æ‰¾åˆ° {len(found_fragments)} ä¸ªå¯èƒ½çš„åˆ†å‰²ç‰‡æ®µ:")
        for i, fragment in enumerate(found_fragments[:5], 1):
            # æå–æ–‡æœ¬å†…å®¹
            text_parts = re.findall(r'<w:t[^>]*>([^<]*)</w:t>', fragment)
            combined_text = ''.join(text_parts)
            print(f"      {i}. {combined_text}")
    else:
        print(f"    æœªæ‰¾åˆ°æ˜æ˜¾çš„åˆ†å‰²ç‰‡æ®µ")
    
    # æŸ¥æ‰¾å•ç‹¬çš„èŠ±æ‹¬å·
    single_braces = re.findall(r'<w:t[^>]*>([^<]*[{}][^<]*)</w:t>', xml_content)
    if single_braces:
        print(f"    åŒ…å«èŠ±æ‹¬å·çš„æ–‡æœ¬èŠ‚ç‚¹: {len(single_braces)} ä¸ª")
        for brace in single_braces[:10]:
            print(f"      - {brace}")

def analyze_xml_structure(xml_content):
    """åˆ†æXMLç»“æ„"""
    print(f"\nğŸ—ï¸  XMLç»“æ„åˆ†æ:")
    
    # ç»Ÿè®¡åŸºæœ¬å…ƒç´ 
    elements = {
        'æ®µè½ (w:p)': len(re.findall(r'<w:p[^>]*>', xml_content)),
        'æ–‡æœ¬è¿è¡Œ (w:r)': len(re.findall(r'<w:r[^>]*>', xml_content)),
        'æ–‡æœ¬ (w:t)': len(re.findall(r'<w:t[^>]*>', xml_content)),
        'è¡¨æ ¼ (w:tbl)': len(re.findall(r'<w:tbl[^>]*>', xml_content)),
        'å†…å®¹æ§ä»¶ (w:sdt)': len(re.findall(r'<w:sdt[^>]*>', xml_content)),
        'ä¹¦ç­¾å¼€å§‹ (w:bookmarkStart)': len(re.findall(r'<w:bookmarkStart[^>]*>', xml_content)),
    }
    
    for element, count in elements.items():
        print(f"  {element}: {count} ä¸ª")

def simulate_system_recognition(xml_content):
    """æ¨¡æ‹Ÿç³»ç»Ÿçš„è¯†åˆ«è¿‡ç¨‹"""
    print(f"\nğŸ¤– æ¨¡æ‹Ÿç³»ç»Ÿè¯†åˆ«è¿‡ç¨‹:")
    
    # æ¨¡æ‹Ÿå½“å‰ç³»ç»Ÿçš„æ­£åˆ™è¡¨è¾¾å¼
    system_patterns = [
        r'\{\{([^}]+)\}\}',  # åŒèŠ±æ‹¬å·
        r'\{([^{}]+)\}',     # å•èŠ±æ‹¬å·ï¼ˆä½†å¯èƒ½æœ‰é™åˆ¶ï¼‰
    ]
    
    print(f"  ğŸ” ä½¿ç”¨ç³»ç»Ÿæ­£åˆ™è¡¨è¾¾å¼:")
    
    for i, pattern in enumerate(system_patterns, 1):
        print(f"    æ¨¡å¼ {i}: {pattern}")
        matches = re.findall(pattern, xml_content)
        
        if matches:
            print(f"      æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…:")
            for match in matches[:10]:
                print(f"        - {match}")
        else:
            print(f"      æœªæ‰¾åˆ°åŒ¹é…")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹æ®Šå­—ç¬¦å¹²æ‰°
    print(f"\n  ğŸ” ç‰¹æ®Šå­—ç¬¦æ£€æŸ¥:")
    
    # æŸ¥æ‰¾åŒ…å«èŠ±æ‹¬å·ä½†å¯èƒ½æœ‰å…¶ä»–å­—ç¬¦çš„æ–‡æœ¬
    complex_patterns = [
        r'<w:t[^>]*>([^<]*\{[^<]*)</w:t>',  # åŒ…å«{çš„æ–‡æœ¬èŠ‚ç‚¹
        r'<w:t[^>]*>([^<]*\}[^<]*)</w:t>',  # åŒ…å«}çš„æ–‡æœ¬èŠ‚ç‚¹
    ]
    
    for pattern in complex_patterns:
        matches = re.findall(pattern, xml_content)
        if matches:
            print(f"    åŒ…å«èŠ±æ‹¬å·çš„æ–‡æœ¬èŠ‚ç‚¹: {len(matches)} ä¸ª")
            for match in matches[:5]:
                print(f"      - {repr(match)}")

def generate_fix_suggestions():
    """ç”Ÿæˆä¿®å¤å»ºè®®"""
    print(f"\nğŸ’¡ ä¿®å¤å»ºè®®:")
    print("=" * 30)
    
    print("1. **æ£€æŸ¥å ä½ç¬¦æ ¼å¼**:")
    print("   - ç¡®ä¿ä½¿ç”¨æ ‡å‡†çš„ {{å­—æ®µå}} æˆ– {å­—æ®µå} æ ¼å¼")
    print("   - é¿å…åœ¨å ä½ç¬¦ä¸­åŒ…å«ç‰¹æ®Šå­—ç¬¦")
    print("   - æ£€æŸ¥æ˜¯å¦æœ‰ä¸å¯è§å­—ç¬¦")
    
    print("\n2. **æ£€æŸ¥XMLåˆ†å‰²é—®é¢˜**:")
    print("   - Wordå¯èƒ½å°†å ä½ç¬¦åˆ†å‰²åˆ°å¤šä¸ª<w:t>èŠ‚ç‚¹ä¸­")
    print("   - éœ€è¦å¢å¼ºç³»ç»Ÿçš„åˆ†å‰²å ä½ç¬¦é‡ç»„èƒ½åŠ›")
    
    print("\n3. **å¢å¼ºè¯†åˆ«ç®—æ³•**:")
    print("   - æ”¹è¿›æ­£åˆ™è¡¨è¾¾å¼ä»¥å¤„ç†æ›´å¤æ‚çš„æƒ…å†µ")
    print("   - æ·»åŠ è·¨èŠ‚ç‚¹å ä½ç¬¦è¯†åˆ«")
    print("   - å¢åŠ è°ƒè¯•æ—¥å¿—ä»¥ä¾¿æ’æŸ¥é—®é¢˜")
    
    print("\n4. **æ¨¡æ¿ä¼˜åŒ–å»ºè®®**:")
    print("   - é‡æ–°è¾“å…¥å ä½ç¬¦ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®")
    print("   - é¿å…åœ¨å ä½ç¬¦å‰åæ·»åŠ é¢å¤–çš„æ ¼å¼")
    print("   - ä½¿ç”¨ç®€å•çš„æ–‡æœ¬æ ¼å¼")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å…·ä½“æ¨¡æ¿å ä½ç¬¦è¯†åˆ«é—®é¢˜åˆ†æ")
    print("ä¸“é—¨åˆ†æä¸Šæ¸¸è½¦æº-å¹¿å·èˆ¶æºï¼ˆé‡‡è´­ï¼‰.docx")
    print("=" * 80)
    
    analyze_specific_template()
    generate_fix_suggestions()
    
    print(f"\n" + "=" * 80)
    print("âœ… åˆ†æå®Œæˆ")

if __name__ == '__main__':
    main()
